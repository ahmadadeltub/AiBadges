import sys
import os

os.environ["QT_QPA_PLATFORM_PLUGIN_PATH"] = "/usr/lib/aarch64-linux-gnu/qt5/plugins/platforms"
os.environ["QT_PLUGIN_PATH"] = "/usr/lib/aarch64-linux-gnu/qt5/plugins"
#os.environ["QT_QPA_PLATFORM"] = "minimal"

from picamera2 import Picamera2  # Import Picamera2 for camera access

import cv2
import csv
import re
import whisper
import mediapipe as mp
import numpy as np
import pandas as pd
import joblib
import threading

from collections import deque  
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import pyttsx3
from gtts import gTTS
from playsound import playsound
import time

from PyQt5.QtWidgets import (QApplication, QWidget, QVBoxLayout, QLabel, QLineEdit,
                             QPushButton, QMessageBox, QListWidget, QHBoxLayout, QFrame, QDialog, QDialogButtonBox, QSizePolicy)
from PyQt5.QtGui import QPixmap, QImage, QFont
from PyQt5.QtCore import Qt, QTimer, QThread, pyqtSignal

from PIL import ImageFont, ImageDraw, Image
import arabic_reshaper
from bidi.algorithm import get_display

def speak_text_async(text):
    def _speak():
        local_engine = pyttsx3.init()
        if re.search(r'[\u0600-\u06FF]', text):
            try:
                tts = gTTS(text=text, lang='ar')
                filename = "temp_arabic.mp3"
                tts.save(filename)
                playsound(filename)
                os.remove(filename)
            except Exception as e:
                print(f"Error in gTTS: {e}")
        else:
            local_engine.say(text)
            local_engine.runAndWait()
    threading.Thread(target=_speak, daemon=True).start()

def putTextArabic(img, text, org, font_path="Amiri-Bold.ttf", font_size=32, color=(0,255,0)):
    reshaped_text = arabic_reshaper.reshape(text)
    # Remove or comment out the bidi reordering
    # bidi_text = get_display(reshaped_text)
    bidi_text = reshaped_text
    
    if len(img.shape) == 3 and img.shape[2] == 3:
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    else:
        img_rgb = img.copy()
    
    pil_im = Image.fromarray(img_rgb)
    draw = ImageDraw.Draw(pil_im)
    try:
        font = ImageFont.truetype(font_path, font_size)
    except IOError:
        font = ImageFont.load_default()
    draw.text(org, bidi_text, font=font, fill=color)
    img = np.array(pil_im)
    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    return img_bgr




def speak_text(engine, text):
    if re.search(r'[\u0600-\u06FF]', text):
        try:
            tts = gTTS(text=text, lang='ar')
            filename = "temp_arabic.mp3"
            tts.save(filename)
            playsound(filename)
            os.remove(filename)
        except Exception as e:
            print(f"Error in gTTS: {e}")
    else:
        engine.say(text)
        engine.runAndWait()



class AddGestureWindow(QWidget):
    GESTURES_FILE = 'gesture_data/gestures.txt'

    def __init__(self):
        super().__init__()
        self.setWindowTitle("Add / Manage Gestures")
        self.setGeometry(0, 0, 650, 850)
        self.init_ui()

    def init_ui(self):
        layout = QVBoxLayout()
        
        # Title Label with increased font size
        self.title_label = QLabel("Manage Gestures")
        self.title_label.setFont(QFont("Arial", 50, QFont.Bold))
        self.title_label.setAlignment(Qt.AlignCenter)
        self.title_label.setStyleSheet("color: #d9f1ff; padding-bottom: 20px;")
        
        # Gesture List with increased font size
        self.gesture_list = QListWidget()
        self.gesture_list.setLayoutDirection(Qt.RightToLeft)
        self.gesture_list.setFont(QFont("Arial", 50))
        self.load_gestures()
        
        # Gesture Input with increased font size
        self.gesture_input = QLineEdit()
        self.gesture_input.setPlaceholderText("Enter gesture name (supports Arabic/English)")
        self.gesture_input.setLayoutDirection(Qt.RightToLeft)
        self.gesture_input.setFont(QFont("Arial", 50))
        
        # Button Layout
        button_layout = QHBoxLayout()
        
        # Add Gesture Button with increased font size
        self.add_button = QPushButton("Add Gesture")
        self.add_button.setFont(QFont("Arial", 50, QFont.Bold))
        self.add_button.setStyleSheet("""
            QPushButton {
                background-color: #2ECC71;
                color: #FFFFFF;
                border-radius: 15px;
                padding: 10px 20px;
            }
            QPushButton:hover {
                background-color: #27AE60;
            }
        """)
        
        # Modify Gesture Button with increased font size
        self.modify_button = QPushButton("Modify Gesture")
        self.modify_button.setFont(QFont("Arial", 30, QFont.Bold))
        self.modify_button.setStyleSheet("""
            QPushButton {
                background-color: #F1C40F;
                color: #FFFFFF;
                border-radius: 8px;
                padding: 10px 20px;
            }
            QPushButton:hover {
                background-color: #D4AC0D;
            }
        """)
        
        # Delete Gesture Button with increased font size
        self.delete_button = QPushButton("Delete Gesture")
        self.delete_button.setFont(QFont("Arial", 30, QFont.Bold))
        self.delete_button.setStyleSheet("""
            QPushButton {
                background-color: #E74C3C;
                color: #FFFFFF;
                border-radius: 8px;
                padding: 10px 20px;
            }
            QPushButton:hover {
                background-color: #C0392B;
            }
        """)
        
        # Add buttons to the button layout
        button_layout.addWidget(self.add_button)
        button_layout.addWidget(self.modify_button)
        button_layout.addWidget(self.delete_button)
        
        # Connect buttons to their respective functions
        self.add_button.clicked.connect(self.add_gesture)
        self.modify_button.clicked.connect(self.modify_gesture)
        self.delete_button.clicked.connect(self.delete_gesture)
        
        # Add widgets to the main layout
        layout.addWidget(self.title_label)
        layout.addWidget(self.gesture_list)
        layout.addWidget(self.gesture_input)
        layout.addLayout(button_layout)
        
        self.setLayout(layout)

    def load_gestures(self):
        self.gesture_list.clear()
        if os.path.exists(self.GESTURES_FILE):
            with open(self.GESTURES_FILE, 'r', encoding='utf-8') as file:
                gestures = file.read().splitlines()
                self.gesture_list.addItems(gestures)

    def add_gesture(self):
        gesture = self.gesture_input.text().strip()
        if not gesture:
            QMessageBox.warning(self, "Error", "Please enter a gesture name.")
            return
        if gesture in self.get_gesture_list():
            QMessageBox.warning(self, "Error", "Gesture already exists.")
            return
        with open(self.GESTURES_FILE, 'a', encoding='utf-8') as file:
            file.write(gesture + '\n')
        self.gesture_list.addItem(gesture)
        self.gesture_input.clear()
        QMessageBox.information(self, "Success", f"Gesture '{gesture}' added successfully.")

    def modify_gesture(self):
        selected_item = self.gesture_list.currentItem()
        if not selected_item:
            QMessageBox.warning(self, "Error", "Please select a gesture to modify.")
            return
        new_gesture = self.gesture_input.text().strip()
        if not new_gesture:
            QMessageBox.warning(self, "Error", "Please enter the new gesture name.")
            return
        if new_gesture in self.get_gesture_list():
            QMessageBox.warning(self, "Error", "Gesture already exists.")
            return
        old_gesture = selected_item.text()
        selected_item.setText(new_gesture)
        self.save_gestures()
        QMessageBox.information(self, "Success", f"Gesture '{old_gesture}' modified to '{new_gesture}'.")

    def delete_gesture(self):
        selected_item = self.gesture_list.currentItem()
        if not selected_item:
            QMessageBox.warning(self, "Error", "Please select a gesture to delete.")
            return
        reply = QMessageBox.question(self, "Delete Gesture", 
                                     f"Are you sure you want to delete '{selected_item.text()}'?",
                                     QMessageBox.Yes | QMessageBox.No, QMessageBox.No)
        if reply == QMessageBox.Yes:
            text = selected_item.text()
            self.gesture_list.takeItem(self.gesture_list.row(selected_item))
            self.save_gestures()
            QMessageBox.information(self, "Success", f"Gesture '{text}' deleted successfully.")

    def save_gestures(self):
        with open(self.GESTURES_FILE, 'w', encoding='utf-8') as file:
            for i in range(self.gesture_list.count()):
                file.write(self.gesture_list.item(i).text() + '\n')

    def get_gesture_list(self):
        return [self.gesture_list.item(i).text() for i in range(self.gesture_list.count())]

# Collect Data Window
class CollectDataWindow(QWidget):
    GESTURES_FILE = 'gesture_data/gestures.txt'

    def __init__(self):
        super().__init__()
        self.setWindowTitle("Collect Data (BGR Fix)")
        self.setGeometry(0, 0, 650, 800)

        self.current_gesture = None
        self.collected_samples = 0
        self.required_samples = 200

        self.hands = mp.solutions.hands.Hands(max_num_hands=1, min_detection_confidence=0.7)

        self.picam2 = None
        self.timer = QTimer(self)
        self.timer.timeout.connect(self.update_frame)

        self.init_ui()

    def init_ui(self):
        layout = QVBoxLayout()
        title_label = QLabel("Collect Data")
        title_label.setFont(QFont("Amiri-Bold", 20, QFont.Bold))
        title_label.setAlignment(Qt.AlignCenter)
        layout.addWidget(title_label)

        self.gesture_list = QListWidget()
        self.load_gestures()

        self.start_button = QPushButton("Start Collecting Data")
        self.start_button.clicked.connect(self.start_collection)

        self.start_button.setStyleSheet("""
            QPushButton {
                background-color: #2ECC71;
                color: #FFFFFF;
                font-size: 16px;
                font-weight: bold;
                border-radius: 8px;
                padding: 10px 20px;
            }
            QPushButton:hover {
                background-color: #27AE60;
            }
        """)
        self.video_label = QLabel("Camera feed will appear here.")
        self.video_label.setAlignment(Qt.AlignCenter)

        layout.addWidget(QLabel("Select a Gesture:"))
        layout.addWidget(self.gesture_list)
        layout.addWidget(self.start_button)
        layout.addWidget(self.video_label)

        self.setLayout(layout)

    def load_gestures(self):
        if os.path.exists(self.GESTURES_FILE):
            with open(self.GESTURES_FILE, 'r', encoding='utf-8') as file:
                gestures = file.read().splitlines()
                self.gesture_list.addItems(gestures)

    def start_collection(self):
        selected_item = self.gesture_list.currentItem()
        if not selected_item:
            QMessageBox.warning(self, "Error", "Please select a gesture first.")
            return
        self.current_gesture = selected_item.text()
        self.collected_samples = 0

        try:
            self.picam2 = Picamera2()
            cam_config = self.picam2.create_preview_configuration(
                main={"format": "BGR888", "size": (640, 480)}
            )
            self.picam2.configure(cam_config)
            self.picam2.start()
        except RuntimeError as e:
            QMessageBox.critical(self, "Error", f"Camera initialization failed: {str(e)}")
            return

        self.timer.start(30)

    def update_frame(self):
        if not self.picam2:
            return
        frame = self.picam2.capture_array()
        if frame is None:
            return
        # Convert BGR to RGB for processing
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(rgb_frame)

        if results and results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                try:
                    landmarks = []
                    for point in hand_landmarks.landmark:
                        landmarks.extend([point.x, point.y, point.z])
                    self.save_landmarks_to_csv(self.current_gesture, landmarks)
                    self.collected_samples += 1
                    mp.solutions.drawing_utils.draw_landmarks(
                        frame,  
                        hand_landmarks,
                        mp.solutions.hands.HAND_CONNECTIONS
                    )
                except Exception as e:
                    print(f"Error processing landmarks: {e}")

        # Embed RTL markers around the Arabic gesture name
        text1 = f"Gesture: \u202B{self.current_gesture}\u202C"
        text2 = f"Samples: {self.collected_samples}/{self.required_samples}"
        frame = putTextArabic(frame, text1, (10, 30), font_size=32, color=(0,255,0))
        frame = putTextArabic(frame, text2, (10, 70), font_size=32, color=(0,255,0))

        show_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        qt_image = QImage(
            show_rgb.data, show_rgb.shape[1], show_rgb.shape[0],
            QImage.Format_RGB888
        )
        self.video_label.setPixmap(QPixmap.fromImage(qt_image))

        if self.collected_samples >= self.required_samples:
            self.finish_collection()

    def finish_collection(self):
        if self.picam2:
            self.picam2.stop()
            self.picam2.close()
            self.picam2 = None
        self.timer.stop()
        QMessageBox.information(
            self,
            "Info",
            f"Collected {self.required_samples} samples for gesture '{self.current_gesture}'."
        )

    def save_landmarks_to_csv(self, gesture_name, landmark_list):
        DATA_DIR = 'gesture_data'
        CSV_FILE = os.path.join(DATA_DIR, 'gesture_landmarks.csv')
        os.makedirs(DATA_DIR, exist_ok=True)
        row = [gesture_name] + landmark_list
        with open(CSV_FILE, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(row)

    def closeEvent(self, event):
        if self.timer.isActive():
            self.timer.stop()
        if self.picam2:
            self.picam2.stop()
            self.picam2.close()
            self.picam2 = None
        self.hands.close()
        event.accept()


class TrainResultDialog(QDialog):
    def __init__(self, message):
        super().__init__()
        self.setWindowTitle("Training Result")
        self.setFixedSize(400, 200)
        layout = QVBoxLayout()
        label = QLabel(message)
        label.setAlignment(Qt.AlignCenter)
        label.setStyleSheet("font-size: 16px; font-weight: bold; color: #ECF0F1;")
        layout.addWidget(label)
        button_box = QDialogButtonBox(QDialogButtonBox.Ok)
        button_box.accepted.connect(self.accept)
        layout.addWidget(button_box)
        self.setStyleSheet("""
        QDialog {
            background-color: #2F3136;
        }
        QDialogButtonBox QPushButton {
            background-color: #7289DA;
            border-radius: 5px;
            color: #FFFFFF;
            padding: 5px 10px;
        }
        QDialogButtonBox QPushButton:hover {
            background-color: #5B6EAE;
        }
        """)
        self.setLayout(layout)

class TrainWindow(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Train Model")
        self.setGeometry(50, 150, 500, 300)
        self.init_ui()

    def init_ui(self):
        layout = QVBoxLayout()
        self.train_button = QPushButton("Start Training")
        self.train_button.clicked.connect(self.train_model)
        self.train_button.setStyleSheet("""
            QPushButton {
                background-color: #2ECC71;
                color: #FFFFFF;
                font-size: 16px;
                font-weight: bold;
                border-radius: 8px;
                padding: 10px 20px;
            }
            QPushButton:hover {
                background-color: #27AE60;
            }
        """)
        layout.addWidget(self.train_button, alignment=Qt.AlignCenter)
        self.setLayout(layout)

    def train_model(self):
        try:
            csv_path = 'gesture_data/gesture_landmarks.csv'
            if not os.path.exists(csv_path):
                QMessageBox.warning(self, "Error", "No data found. Collect gesture data first.")
                return
            data = pd.read_csv(csv_path)
            X = data.iloc[:, 1:].values
            y = data.iloc[:, 0].values
            le = LabelEncoder()
            y_encoded = le.fit_transform(y)
            X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)
            model = Sequential([
                Input(shape=(X_train.shape[1],)),
                Dense(128, activation='relu'),
                Dropout(0.2),
                Dense(64, activation='relu'),
                Dense(len(np.unique(y_encoded)), activation='softmax')
            ])
            model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            early_stopping = EarlyStopping(monitor='val_loss', patience=3)
            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)
            model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, callbacks=[early_stopping, reduce_lr])
            os.makedirs('models', exist_ok=True)
            model.save('models/gesture_recognition_model.keras')
            joblib.dump(scaler, 'models/scaler.save')
            joblib.dump(le, 'models/label_encoder.save')
            dialog = TrainResultDialog("Model trained and saved successfully!")
            dialog.exec_()
        except Exception as e:
            QMessageBox.critical(self, "Error", f"Training failed: {str(e)}")

class TranslateWindow(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Translate Gestures")
        self.setGeometry(0, 0, 650, 850)
        self.init_ui()
        self.hands = mp.solutions.hands.Hands(max_num_hands=1, min_detection_confidence=0.7)
        self.engine = pyttsx3.init()

        voices = self.engine.getProperty('voices')
        arabic_keywords = ["Arabic", "Naayf", "Zehra", "Tagh"]
        self.arabic_voice_found = False
        for voice in voices:
            if any(keyword.lower() in voice.name.lower() for keyword in arabic_keywords):
                self.engine.setProperty('voice', voice.id)
                self.arabic_voice_found = True
                break

        self.model = load_model('models/gesture_recognition_model.keras')
        self.scaler = joblib.load('models/scaler.save')
        self.label_encoder = joblib.load('models/label_encoder.save')

        self.picam2 = Picamera2()
        self.picam2.configure(self.picam2.create_preview_configuration())
        self.picam2.start()

        self.timer = QTimer(self)
        self.timer.timeout.connect(self.update_frame)
        self.timer.start(30)
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_hands = mp.solutions.hands
        self.last_gesture = None

    def init_ui(self):
        main_layout = QVBoxLayout()
        top_button_layout = QHBoxLayout()
        self.back_button = QPushButton(" Back")
        self.back_button.setFont(QFont("Arial", 100, QFont.Bold))
        self.back_button.setStyleSheet("""
            QPushButton {
                background-color: #F1C40F;
                color: #FFFF;
                border-radius: 16px;
                padding: 20px 30px;
            }
            QPushButton:hover {
                background-color: #D8AC1D;
            }
        """)
        
        self.exit_button = QPushButton(" Exit")
        self.exit_button.setFont(QFont("Arial", 50, QFont.Bold))
        self.exit_button.setStyleSheet("""
            QPushButton {
                background-color: #ff0000;
                color: #FFFFFF;
                border-radius: 16px;
                padding: 20px 30px;
            }
            QPushButton:hover {
                background-color: #ff0000;
            }
        """)
        top_button_layout.addWidget(self.back_button)
        top_button_layout.addWidget(self.exit_button)
        main_layout.addLayout(top_button_layout)
        self.video_label = QLabel()
        main_layout.addWidget(self.video_label)
        self.setLayout(main_layout)

        self.back_button.clicked.connect(self.close)
        self.exit_button.clicked.connect(QApplication.quit)

    def update_frame(self):
        frame = self.picam2.capture_array()
        if frame is None:
            return
        frame = cv2.flip(frame, 1)
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(frame_rgb)
        gesture = "Not Detected"
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                self.mp_drawing.draw_landmarks(frame_rgb, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)
                landmarks = [lm for point in hand_landmarks.landmark for lm in (point.x, point.y, point.z)]
                X = self.scaler.transform(np.array(landmarks).reshape(1, -1))
                prediction = self.model.predict(X)
                class_id = np.argmax(prediction)
                gesture = self.label_encoder.inverse_transform([class_id])[0]
                if self.last_gesture != gesture:
                    speak_text_async(gesture)
                    self.last_gesture = gesture
        frame_rgb = putTextArabic(frame_rgb, f"Gesture: {gesture}", (40, 80), font_size=32, color=(0, 255, 0))
        qt_image = QImage(frame_rgb.data, frame_rgb.shape[1], frame_rgb.shape[0], QImage.Format_RGB888)
        self.video_label.setPixmap(QPixmap.fromImage(qt_image))

    def closeEvent(self, event):
        self.timer.stop()
        self.picam2.stop()
        self.hands.close()
        event.accept()

class ReverseTranslateWindow(QWidget):
    GESTURES_FILE = 'gesture_data/gestures.txt'
    GESTURE_IMAGES_DIR = 'gesture_images'  # Directory containing gesture images

    def __init__(self):
        super().__init__()
        self.setWindowTitle("Reverse Translate (AI-Powered Speech to Gesture)")
        self.init_ui()
        self.load_gestures()

        self.whisper_model = whisper.load_model("small")  # Downloaded at first use

        QTimer.singleShot(500, self.start_listening)

    def start_listening(self):
        self.instruction_label.setText("Please say the gesture name...")
        self.status_label.setText("Adjusting microphone noise...")
        QApplication.processEvents()

        self.adjust_mic_noise(1.0)

        self.status_label.setText("Listening (speak now)...")
        QApplication.processEvents()
        QTimer.singleShot(100, self.capture_speech)

    def adjust_mic_noise(self, duration=1.0):
        pass

    def capture_speech(self):
        audio_file = "temp_speech_input.wav"
        if not self.record_audio(audio_file, record_seconds=4):
            self.status_label.setText("No audio captured. Please try again.")
            return

        self.status_label.setText("Processing with Whisper...")
        QApplication.processEvents()

        recognized_text = self.run_whisper(audio_file)
        if recognized_text is None or recognized_text.strip() == "":
            self.status_label.setText("Could not understand speech. Please try again.")
            self.instruction_label.setText("No speech recognized.")
            return

        self.show_gesture_for_word(recognized_text)

    def record_audio(self, filename, record_seconds=4):
        import pyaudio
        import wave

        CHUNK = 1024
        FORMAT = pyaudio.paInt16
        CHANNELS = 1
        RATE = 16000
        p = pyaudio.PyAudio()

        stream = p.open(format=FORMAT, channels=CHANNELS,
                        rate=RATE, input=True, frames_per_buffer=CHUNK)

        frames = []
        for i in range(0, int(RATE / CHUNK * record_seconds)):
            try:
                data = stream.read(CHUNK)
                frames.append(data)
            except Exception as e:
                print(f"Error recording audio: {e}")
                break

        stream.stop_stream()
        stream.close()
        p.terminate()

        if len(frames) == 0:
            return False

        wf = wave.open(filename, 'wb')
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(p.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))
        wf.close()
        return True

    def run_whisper(self, audio_file):
        result = self.whisper_model.transcribe(audio_file, fp16=False)
        return result["text"].lower().strip()

    def show_gesture_for_word(self, word):
        # Normalize English word by removing punctuation and converting to lowercase
        normalized_word = re.sub(r'[^\w\s]', '', word).strip().lower()

        # Check if the word contains Arabic characters
        if re.search(r'[\u0600-\u06FF]', word):
            # Arabic logic
            if word in self.gestures:
                image_path = os.path.join(self.GESTURE_IMAGES_DIR, f"{word}.png")
                if os.path.exists(image_path):
                    pixmap = QPixmap(image_path)
                    self.image_label.setPixmap(pixmap.scaled(400, 400, Qt.KeepAspectRatio, Qt.SmoothTransformation))
                    self.instruction_label.setText(f"Understood: '{word}'")
                    self.status_label.setText("Gesture found!")
                    QApplication.processEvents()
                    QTimer.singleShot(2000, self.close)
                else:
                    self.status_label.setText("No matching image found.")
                    self.instruction_label.setText(f"No image for '{word}'. Try another gesture.")
            else:
                self.status_label.setText("Unknown gesture.")
                self.instruction_label.setText(f"'{word}' does not match any known gesture. Try again.")
        else:
            # English logic: try to match normalized word with gesture list
            matched_gesture = None
            for gesture in self.gestures:
                if gesture.lower() == normalized_word:
                    matched_gesture = gesture
                    break
            if matched_gesture:
                image_path = os.path.join(self.GESTURE_IMAGES_DIR, f"{matched_gesture}.png")
                if os.path.exists(image_path):
                    pixmap = QPixmap(image_path)
                    self.image_label.setPixmap(pixmap.scaled(400, 400, Qt.KeepAspectRatio, Qt.SmoothTransformation))
                    self.instruction_label.setText(f"Understood: '{word}'")
                    self.status_label.setText("Gesture found!")
                    QApplication.processEvents()
                    QTimer.singleShot(2000, self.close)
                else:
                    self.status_label.setText("No matching image found.")
                    self.instruction_label.setText(f"No image for '{word}'. Try another gesture.")
            else:
                self.status_label.setText("Unknown gesture.")
                self.instruction_label.setText(f"'{word}' does not match any known gesture. Try again.")

    def init_ui(self):
        self.main_layout = QVBoxLayout()

        # Title
        title_label = QLabel("Reverse Translate")
        title_label.setFont(QFont("Arial", 24, QFont.Bold))
        title_label.setAlignment(Qt.AlignCenter)
        self.main_layout.addWidget(title_label)

        # Instruction label
        self.instruction_label = QLabel("Please say the gesture name...")
        self.instruction_label.setFont(QFont("Arial", 16))
        self.instruction_label.setAlignment(Qt.AlignCenter)
        self.main_layout.addWidget(self.instruction_label)

        # Microphone and status layout
        mic_layout = QHBoxLayout()
        self.mic_label = QLabel()
        if os.path.exists("mic.png"):
            mic_pixmap = QPixmap("mic.png").scaled(150, 150, Qt.KeepAspectRatio, Qt.SmoothTransformation)
            self.mic_label.setPixmap(mic_pixmap)
        else:
            self.mic_label.setText("ðŸŽ¤")
            self.mic_label.setAlignment(Qt.AlignCenter)
            font = self.mic_label.font()
            font.setPointSize(48)
            self.mic_label.setFont(font)

        mic_layout.addWidget(self.mic_label, alignment=Qt.AlignCenter)

        self.status_label = QLabel("Preparing to listen...")
        self.status_label.setAlignment(Qt.AlignCenter)
        self.status_label.setFont(QFont("Arial", 14, QFont.Normal))
        mic_layout.addWidget(self.status_label)
        self.main_layout.addLayout(mic_layout)

        self.image_label = QLabel()
        self.image_label.setAlignment(Qt.AlignCenter)
        self.main_layout.addWidget(self.image_label)

        # Buttons (Retry and Close)
        button_layout = QHBoxLayout()
        self.retry_button = QPushButton("â†» Retry")
        self.close_button = QPushButton("â¨‰ Close")

        self.retry_button.setStyleSheet("""
            QPushButton {
                background-color: #3498DB;
                color: #ECF0F1;
                font-size: 16px;
                font-weight: bold;
                border-radius: 8px;
                padding: 8px 20px;
            }
            QPushButton:hover {
                background-color: #2980B9;
            }
        """)

        self.close_button.setStyleSheet("""
            QPushButton {
                background-color: #E74C3C;
                color: #ECF0F1;
                font-size: 16px;
                font-weight: bold;
                border-radius: 8px;
                padding: 8px 20px;
            }
            QPushButton:hover {
                background-color: #C0392B;
            }
        """)

        self.retry_button.clicked.connect(self.start_listening)
        self.close_button.clicked.connect(self.close)

        button_layout.addWidget(self.retry_button)
        button_layout.addWidget(self.close_button)
        self.main_layout.addLayout(button_layout)

        # Set the layout
        self.setLayout(self.main_layout)

    def load_gestures(self):
        self.gestures = []
        if os.path.exists(self.GESTURES_FILE):
            with open(self.GESTURES_FILE, 'r', encoding='utf-8') as file:
                self.gestures = file.read().splitlines()

    def closeEvent(self, event):
        event.accept()

class ContinuousTranslateWindow(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Continuous Translate Gestures")
        self.setGeometry(100, 100, 800, 600)
        self.init_ui()

        self.hands = mp.solutions.hands.Hands(max_num_hands=1, min_detection_confidence=0.7)
        self.engine = pyttsx3.init()
        voices = self.engine.getProperty('voices')
        arabic_keywords = ["Arabic", "Naayf", "Zehra", "Tagh"]
        for voice in voices:
            if any(keyword.lower() in voice.name.lower() for keyword in arabic_keywords):
                self.engine.setProperty('voice', voice.id)
                break

        self.model = load_model('models/gesture_recognition_model.keras')
        self.scaler = joblib.load('models/scaler.save')
        self.label_encoder = joblib.load('models/label_encoder.save')

        self.picam2 = Picamera2()
        self.picam2.configure(self.picam2.create_preview_configuration())
        self.picam2.start()

        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_hands = mp.solutions.hands

        self.gesture_sequence = []
        self.last_prediction = None
        self.no_hand_counter = 0

        self.timer = QTimer(self)
        self.timer.timeout.connect(self.update_frame)
        self.timer.start(30)

    def init_ui(self):
        main_layout = QVBoxLayout()
        top_button_layout = QHBoxLayout()
        self.back_button = QPushButton("â† Back")
        self.exit_button = QPushButton("â¨‰ Exit")
        top_button_layout.addWidget(self.back_button)
        top_button_layout.addStretch()
        top_button_layout.addWidget(self.exit_button)
        self.video_label = QLabel()
        self.sequence_label = QLabel("Detected Sequence: ")
        self.sequence_label.setAlignment(Qt.AlignCenter)
        self.sequence_label.setStyleSheet("font-size: 24px; color: #FFFFFF;")
        main_layout.addLayout(top_button_layout)
        main_layout.addWidget(self.video_label)
        main_layout.addWidget(self.sequence_label)
        self.setLayout(main_layout)

        self.back_button.clicked.connect(self.close)
        self.exit_button.clicked.connect(QApplication.quit)

    def update_frame(self):
        frame = self.picam2.capture_array()
        if frame is None:
            return
        frame = cv2.flip(frame, 1)
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(frame_rgb)
        if results.multi_hand_landmarks:
            self.no_hand_counter = 0
            for hand_landmarks in results.multi_hand_landmarks:
                self.mp_drawing.draw_landmarks(frame_rgb, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)
                landmarks = [lm for point in hand_landmarks.landmark for lm in (point.x, point.y, point.z)]
                X = self.scaler.transform(np.array(landmarks).reshape(1, -1))
                prediction = self.model.predict(X)
                class_id = np.argmax(prediction)
                gesture = self.label_encoder.inverse_transform([class_id])[0]
                if gesture != self.last_prediction:
                    self.last_prediction = gesture
                    self.gesture_sequence.append(gesture)
                    self.sequence_label.setText("Detected Sequence: " + " ".join(self.gesture_sequence))
                    speak_text_async(gesture)
        else:
            self.no_hand_counter += 1
            if self.no_hand_counter > 30 and self.gesture_sequence:
                self.gesture_sequence = []
                self.sequence_label.setText("Detected Sequence: ")
        frame_rgb = putTextArabic(frame_rgb, "Continuous Detection", (40, 40), font_size=32, color=(0,255,0))
        qt_image = QImage(frame_rgb.data, frame_rgb.shape[1], frame_rgb.shape[0], QImage.Format_RGB888)
        self.video_label.setPixmap(QPixmap.fromImage(qt_image))

    def closeEvent(self, event):
        self.timer.stop()
        self.picam2.stop()
        self.hands.close()
        event.accept()
class MainMenu(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("AI Smart Badge")
        # Set desired geometry (width reduced as needed)
        self.setGeometry(0, 0, 600, 1280)
        self.init_ui()

    def init_ui(self):
        # Create a main vertical layout
        main_layout = QVBoxLayout()

        # Horizontal layout for logos at the top
        logo_layout = QHBoxLayout()
        self.left_logo_label = QLabel()
        if os.path.exists("qstss.png"):
            left_pixmap = QPixmap("qstss.png")
            # Adjust size as needed
            self.left_logo_label.setPixmap(left_pixmap.scaled(300, 350, Qt.KeepAspectRatio, Qt.SmoothTransformation))
        self.left_logo_label.setAlignment(Qt.AlignLeft | Qt.AlignVCenter)

        self.right_logo_label = QLabel()
        if os.path.exists("hamdan.png"):
            right_pixmap = QPixmap("hamdan.png")
            # Adjust size as needed
            self.right_logo_label.setPixmap(right_pixmap.scaled(300, 350, Qt.KeepAspectRatio, Qt.SmoothTransformation))
        self.right_logo_label.setAlignment(Qt.AlignRight | Qt.AlignVCenter)

        # Add logos to the horizontal layout with stretches for spacing
        logo_layout.addWidget(self.left_logo_label)
        logo_layout.addStretch()
        logo_layout.addWidget(self.right_logo_label)

        # Create the title label and style it
        self.title_label = QLabel("AI Smart Badge Software")
        self.title_label.setStyleSheet("""
            color: #d9f1ff; 
            font-size: 50px; 
            font-weight: bold; 
            padding-top: 20px;
        """)
        main_layout.addSpacing(0)

        self.title_label.setAlignment(Qt.AlignCenter)

        # Add the logo layout and title label to the main vertical layout
        main_layout.addLayout(logo_layout)
        main_layout.addSpacing(0)
        main_layout.addWidget(self.title_label)

        # Continue with the rest of the UI setup (canvas, buttons, etc.)
        self.canvas = QFrame(self)
        self.canvas.setObjectName("canvasFrame")
        self.canvas.setFrameShape(QFrame.StyledPanel)
        self.canvas.setFrameShadow(QFrame.Raised)

        canvas_layout = QVBoxLayout(self.canvas)

        self.add_gesture_button = self.create_button(
            text=" Add / Manage Gestures",
            color="#9B59B6",
            hover_color="#8E44AD",
            action=self.open_add_gesture
        )
        self.collect_data_button = self.create_button(
            text="  Collect Data",
            color="#3498DB",
            hover_color="#2980B9",
            action=self.open_collect_data
        )
        self.train_model_button = self.create_button(
            text=" Train Model",
            color="#2ECC71",
            hover_color="#27AE60",
            action=self.open_train_model
        )
        self.translate_button = self.create_button(
            text=" Translate Gestures",
            color="#F39C12",
            hover_color="#E67E22",
            action=self.open_translate_window
        )
        self.reverse_translate_button = self.create_button(
            text="Reverse Translate",
            color="#8E84AD",
            hover_color="#7D3C98",
            action=self.open_reverse_translate
        )
        self.continuous_translate_button = self.create_button(
            text=" Continuous Translate",
            color="#1ABC9C",
            hover_color="#16A085",
            action=self.open_continuous_translate
        )
        self.exit_button = self.create_button(
            text=" Exit",
            color="#E74C3C",
            hover_color="#C0392B",
            action=self.confirm_exit
        )

        canvas_layout.addWidget(self.add_gesture_button)
        canvas_layout.addWidget(self.collect_data_button)
        canvas_layout.addWidget(self.train_model_button)
        canvas_layout.addWidget(self.translate_button)
        canvas_layout.addWidget(self.reverse_translate_button)
        canvas_layout.addWidget(self.continuous_translate_button)
        canvas_layout.addWidget(self.exit_button)

        main_layout.addWidget(self.canvas)

        self.setLayout(main_layout)

    def create_button(self, text, color, hover_color, action):
        btn = QPushButton(text)
        btn.setMinimumHeight(130)
        btn.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Fixed)
        btn.setStyleSheet(f"""
            QPushButton {{
                background-color: {color};
                border-radius: 8px;
                padding: 4px 10px;
                color: #ECF0F1;
                font-size: 30px;
                font-weight: bold;
                text-align: center;
            }}
            QPushButton:hover {{
                background-color: {hover_color};
            }}
        """)
        btn.clicked.connect(action)
        btn.setIconSize(btn.sizeHint())
        return btn

    def confirm_exit(self):
        reply = QMessageBox.question(self, 'Exit', "Are you sure you want to exit?",
                                     QMessageBox.Yes | QMessageBox.No, QMessageBox.No)
        if reply == QMessageBox.Yes:
            QApplication.quit()

    def open_add_gesture(self):
        self.add_window = AddGestureWindow()
        self.add_window.show()

    def open_collect_data(self):
        self.collect_window = CollectDataWindow()
        self.collect_window.show()

    def open_train_model(self):
        self.train_window = TrainWindow()
        self.train_window.show()

    def open_translate_window(self):
        self.translate_window = TranslateWindow()
        self.translate_window.show()

    def open_reverse_translate(self):
        self.reverse_window = ReverseTranslateWindow()
        self.reverse_window.show()

    def open_continuous_translate(self):
        self.continuous_window = ContinuousTranslateWindow()
        self.continuous_window.show()

if __name__ == "__main__":
    from PyQt5.QtCore import QCoreApplication

    # Set library paths to include the directory containing Qt plugins
    QCoreApplication.setLibraryPaths(["/usr/lib/aarch64-linux-gnu/qt5/plugins"])

    app = QApplication(sys.argv)
    app.setStyleSheet("""
    QWidget {
        background-color: #123499;
        color: #FFFFFF;
        font-family: Arial;
        font-size: 14px;
    }
    QLabel {
        font-size: 20px;
        font-weight: bold;
        color: #FFFFFF;
    }
    QLineEdit {
        background-color: #40444B;
        border: 1px solid #5B6EAE;
        border-radius: 5px;
        padding: 5px;
        color: #FFFFFF;
    }
    QComboBox {
        background-color: #40444B;
        border: 1px solid #5B6EAE;
        border-radius: 5px;
        padding: 5px;
        color: #FFFFFF;
    }
    QComboBox QAbstractItemView {
        background-color: #40444B;
        selection-background-color: #5B6EAE;
        color: #FFFFFF;
    }
    QListWidget {
        background-color: #40444B;
        border: 1px solid #5B6EAE;
        border-radius: 5px;
        color: #FFFFFF;
    }
    QFrame#canvasFrame {
        background-color: #23272A;
        border: 2px solid #5B6EAE;
        border-radius: 10px;
        padding: 10px;
    }
    QFrame#collectDataFrame {
        background-color: #23272A;
        border: 2px solid #5B6EAE;
        border-radius: 10px;
        padding: 15px;
    }
    """)
    window = MainMenu()
    window.show()
    sys.exit(app.exec_())
